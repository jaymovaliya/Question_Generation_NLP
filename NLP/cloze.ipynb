{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->image processing forms core research area within engineering and computer science disciplines too.image processing basically includes the following three steps.importing the image with optical scanner or by digital photography.\n",
      "->image processing is a method to convert an image into digital form and perform some operations on it, in order to get an enhanced image or to extract some useful information from it.\n",
      "->the two types of methods used for image processing are analog and digital image processing.\n",
      "->usually image processing system includes treating images as two dimensional signals while applying already set signal processing methods to them.it is among rapidly growing technologies today, with its applications in various aspects of a business.\n",
      "->it is a type of signal dispensation in which input is image, like video frame or photograph and output may be image or characteristics associated with that image.\n"
     ]
    }
   ],
   "source": [
    "lst = \"Image processing is a method to convert an image into digital form and perform some operations on it, in order to get an enhanced image or to extract some useful information from it. It is a type of signal dispensation in which input is image, like video frame or photograph and output may be image or characteristics associated with that image. Usually Image Processing system includes treating images as two dimensional signals while applying already set signal processing methods to them.It is among rapidly growing technologies today, with its applications in various aspects of a business. Image Processing forms core research area within engineering and computer science disciplines too.Image processing basically includes the following three steps.Importing the image with optical scanner or by digital photography. Analyzing and manipulating the image which includes data compression and image enhancement and spotting patterns that are not to human eyes like satellite photographs. Output is the last stage in which result can be altered image or report that is based on image analysis. The two types of methods used for Image Processing are Analog and Digital Image Processing. Analog or visual techniques of image processing can be used for the hard copies like printouts and photographs.Image analysts use various fundamentals of interpretation while using these visual techniques. The image processing is not just confined to area that has to be studied but on knowledge of analyst. Association is another important tool in image processing through visual techniques. So analysts apply a combination of personal knowledge and collateral data to image processing. Digital Processing techniques help in manipulation of the digital images by using computers. As raw data from imaging sensors from satellite platform contains deficiencies. To get over such flaws and to get originality of information, it has to undergo various phases of processing. The three general phases that all types of data have to undergo while using digital technique are Pre- processing, enhancement and display, information extraction.\"\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import operator\n",
    "import string\n",
    "\n",
    "\n",
    "def TermFreq(wordDict, text):\n",
    "    tfDict = {}\n",
    "    totlen = len(text)\n",
    "    for w, cnt in wordDict.items():\n",
    "        tfDict[w] = cnt / float(totlen)\n",
    "    return tfDict                           \n",
    "\n",
    "\n",
    "f = lst\n",
    "content = f.lower()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"’\")\n",
    "data = word_tokenize(content)\n",
    "sentenses = sent_tokenize(content)\n",
    "# print(sentenses)\n",
    "filter_text = []\n",
    "for w in data:\n",
    "    if w not in stop_words and w not in string.punctuation:\n",
    "        filter_text.append(w)\n",
    "wordDict = dict.fromkeys(filter_text, 0)\n",
    "for w in filter_text:\n",
    "    wordDict[w] += 1\n",
    "# print(wordDict)\n",
    "tf = TermFreq(wordDict, filter_text)\n",
    "sorted_tf = sorted(tf.items(), key=operator.itemgetter(1), reverse=True)[:10]\n",
    "# print(sorted_tf)\n",
    "senDict = dict.fromkeys(sentenses, 0)\n",
    "for s in sentenses:\n",
    "    tok = word_tokenize(s)\n",
    "    weight = 0\n",
    "    for w in tok:\n",
    "        if (w in tf.keys()):\n",
    "            weight += tf[w]\n",
    "    senDict[s] = weight\n",
    "# print(senDict)\n",
    "sorted_sd = sorted(senDict.items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "for i in sorted_sd:\n",
    "    print(\"->\" + i[0])\n",
    "# sorted_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_replace(sentence,word):\n",
    "    return sentence.replace(word,\"____\")\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentencez = \"\"\n",
    "with open(\"data.txt\",\"r\") as f:\n",
    "    sentencez = f.read().lower()\n",
    "sentencez = sent_tokenize(sentencez)\n",
    "\n",
    "# essays = \"Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data.\"\n",
    "# Stop words\n",
    "postagz = []\n",
    "for i in sentencez:\n",
    "    tokens = nltk.word_tokenize(i)\n",
    "#     stopwords = nltk.corpus.stopwords.words('english')\n",
    "#     first_text_list_cleaned = [word for word in tokens if word.lower() not in stopwords]\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    postagz.append(tagged)\n",
    "    \n",
    "# tokens = nltk.word_tokenize(essays)\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "# first_text_list_cleaned = [word for word in tokens if word.lower() not in stopwords]\n",
    "\n",
    "# # tagged = nltk.pos_tag(tokens)\n",
    "# # nouns = [word for word,pos in tagged if (pos == 'NN' or pos == 'JJ')]\n",
    "# # find_replace(essays,nouns[0])\n",
    "# # first_text_list_cleaned = [word for word in tagged if word.lower() not in stopwords]\n",
    "# # nouns\n",
    "# # tagged\n",
    "# # first_text_list_cleaned\n",
    "\n",
    "# tok = first_text_list_cleaned\n",
    "# # Pos - Tags\n",
    "# tagged = nltk.pos_tag(tok)\n",
    "# nouns = [word for word,pos in tagged if (pos == 'NN')]\n",
    "# # nouns\n",
    "# tagged\n",
    "postagz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences without stopwords.................Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# essays = \"Time travel is the concept of movement between certain points in time, analogous to movement between different points in space by an object or a person, typically using a hypothetical device known as a time machine.\"\n",
    "# essays = \"It is uncertain if time travel to the past is physically possible. Forward time travel,outside the usual sense of the perception of time, is an extensively-observed phenomenon and well-understood within the framework of special relativity and general relativity.\"\n",
    "# essays = \"Traveling to an arbitrary point in spacetime has a very limited support in theoretical physics, and usually only connected with quantum mechanic or wormholes, also known as Einstein-Rosen bridges.\"\n",
    "# essays = \"According to Einstein’s theory of special relativity, when you travel at speeds approaching the speed of light, time slows down for you relative to the outside world.\"\n",
    "# essays = \"Using twin atomic clocks (one flown in a jet aircraft, the other stationary on Earth) physicists have shown that a flying clock ticks slower, because of its speed.\"\n",
    "# essays = \"Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data.\"\n",
    "# essays = \"Bioinformatics has been used for in silico analyses of biological queries using mathematical and statistical techniques.\"\n",
    "# essays = \"Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis 'pipelines' that are repeatedly used, particularly in the field of genomics.\"\n",
    "# essays =\"common uses of bioinformatics include the identification of candidates genes and single nucleotide polymorphisms (SNPs).\"\n",
    "# essays = \"In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data.\"\n",
    "# essays = \"IoT is a transformational force that can help companies improve performance through IoT analytics and IoT Security to deliver better results.\"\n",
    "# essays = \"The embedded technology in the objects helps them to interact with internal states or the external environment, which in turn affects the decisions taken.\"\n",
    "tokens = nltk.word_tokenize(essays.lower())\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "first_text_list_cleaned = [word for word in tokens if word.lower() not in stopwords]\n",
    "tok = first_text_list_cleaned\n",
    "withouttagged = nltk.pos_tag(tok)\n",
    "# prtaggedint(\"Stopwords \\n\",tagged)\n",
    "# print(\"No STOP\\n\",withouttagged)\n",
    "tagged,withouttagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = tagged\n",
    "saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditionals Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('referred', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('as', 'IN'),\n",
       " ('memoization', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "def sentence_spacenizer(x):\n",
    "    length = len(x)\n",
    "    \n",
    "    # Starting from \"of\" following \"NN\"\n",
    "    for i in range(length):\n",
    "        if  x[i][0] == \"of\" and x[i+1][1] == \"NN\":\n",
    "            return (i+1)\n",
    "        \n",
    "    # Starting from \"JJ\"\n",
    "    for i in range(length):\n",
    "        if i+1 < length and x[i][1] == \"JJ\":\n",
    "            if i+2 < length and x[i+1][1] == \"CC\":\n",
    "                if x[i+2][1] == \"JJ\":\n",
    "                    return i,(i+2)\n",
    "                \n",
    "            if i+2 < length and x[i+1][1] == \"NN\":\n",
    "                if i+3 < length and x[i+2][1] == \"CC\":\n",
    "                    if i+4 < length and x[i+3][1] == \"JJ\": \n",
    "                        if (x[i+4][1] == \"NN\" or x[i+4][1] == \"NNS\"):\n",
    "                            return i,i+1,i+3,i+4\n",
    "                    if x[i+3][1] == \"NNS\":\n",
    "                        return i,i+1,i+3\n",
    "                \n",
    "                if x[i+2][1] == \"NN\" or x[i+2][1] == \"NNS\":\n",
    "                        return i,i+1,i+2\n",
    "                \n",
    "                return i,i+1\n",
    "    \n",
    "    for i in range(length):\n",
    "        if i+2 < length and x[i][0] == \"referred\" and (x[i+1][0] == \"to\" or x[i+1][0] == \"as\"):\n",
    "            if i+3 < length and x[i+2][1] == \"NN\":\n",
    "                if x[i+3][1] == \"NN\":\n",
    "                    return i+2,i+3\n",
    "                else:\n",
    "                    return i+2\n",
    "                \n",
    "        if i+2 < length and x[i][0] == \"known\" and x[i+1][0] == \"as\":\n",
    "            if i+3 < length and x[i+2][1] == \"NN\":\n",
    "                if x[i+3][1] == \"NN\":\n",
    "                    return i+2,i+3\n",
    "                else:\n",
    "                    return i+2\n",
    "                \n",
    "# returns sentence with blanks\n",
    "def withblank(tagged):\n",
    "    y = sentence_spacenizer(tagged)\n",
    "    if y == None:\n",
    "        return\n",
    "    if type(y) == type(1):\n",
    "        sen = \"\"\n",
    "        for i in range(len(tagged)):\n",
    "            if i != y:\n",
    "                sen += tagged[i][0] + \" \"\n",
    "            else:\n",
    "                sen += \"_________\" + \" \"\n",
    "    else:\n",
    "        sen = \"\"\n",
    "        for i in range(len(tagged)):\n",
    "            if i not in y:\n",
    "                sen += tagged[i][0] + \" \"\n",
    "            else:\n",
    "                sen += \"_________\" + \" \"\n",
    "        return sen\n",
    "\n",
    "\n",
    "# import nltk\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# # essays = \"common uses of bioinformatics include the identification of candidates genes and single nucleotide polymorphisms (SNPs).\"\n",
    "# essays = \"a b c\"\n",
    "# tokens = nltk.word_tokenize(essays.lower())\n",
    "# tagged = nltk.pos_tag(tokens)\n",
    "# print(withblank(tagged))\n",
    "# y = sentence_spacenizer(tagged)\n",
    "# sen = \"\"\n",
    "# for i in range(len(tagged)):\n",
    "#     if i not in y:\n",
    "#         sen += tagged[i][0] + \" \"\n",
    "#     else:\n",
    "#         sen += \"_________\"\n",
    "# print(sen)\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "# first_text_list_cleaned = [word for word in tokens if word.lower() not in stopwords]\n",
    "# tok = first_text_list_cleaned\n",
    "\n",
    "# withouttagged = nltk.pos_tag(tok)\n",
    "# # prtaggedint(\"Stopwords \\n\",tagged)\n",
    "# # print(\"No STOP\\n\",withouttagged)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an ​ artificial _________ _________ ( ann ) learning algorithm , usually called `` neural network '' ( nn ) , is a learning algorithm that is vaguely inspired by ​ biological neural networks​ . \n",
      "machine learning is sometimes conflated with ​ data mining​ , where the _________ _________ focuses more on ​ exploratory data analysis​ and is known as ​ unsupervised learning​ . \n"
     ]
    }
   ],
   "source": [
    "lst = \"\"\n",
    "with open(\"data.txt\",\"r\") as f:\n",
    "    lst = f.read().lower()\n",
    "\n",
    "# lst = \"Analyze the problem and see the order in which the sub-problems are solved and start solving from the trivial subproblem, up towards the given problem. In this process, it is guaranteed that the subproblems are solved before solving the problem and thus, it is referred to as Dynamic Programming. Start solving the given problem by breaking it down. If you see that the problem has been solved already, then just return the saved answer. If it has not been solved, solve it and save the answer. This is usually easy to think of and very intuitive and thus, it is referred to as Memoization. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. Also, the optimal solutions to the subproblems contribute to the optimal solution of the given problem. There are two ways of doing this. Start solving the given problem by breaking it down. If you see that the problem has been solved already, then just return the saved answer. If it has not been solved, solve it and save the answer. This is usually easy to think of and very intuitive. This is referred to as Memoization. Analyze the problem and see the order in which the sub-problems are solved and start solving from the trivial subproblem, up towards the given problem.If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems. In the optimization literature this relationship is called the Bellman equation. Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure. \"\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import operator\n",
    "import string\n",
    "\n",
    "\n",
    "def TermFreq(wordDict, text):\n",
    "    tfDict = {}\n",
    "    totlen = len(text)\n",
    "    for w, cnt in wordDict.items():\n",
    "        tfDict[w] = cnt / float(totlen)\n",
    "    return tfDict                           \n",
    "\n",
    "f = lst\n",
    "content = f.lower()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"’\")\n",
    "data = word_tokenize(content)\n",
    "sentenses = sent_tokenize(content)\n",
    "# for i in sentenses:\n",
    "#     tokens = nltk.word_tokenize(i)\n",
    "#     tagged = nltk.pos_tag(tokens)\n",
    "#     print(tagged)\n",
    "filter_text = []\n",
    "for w in data:\n",
    "    if w not in stop_words and w not in string.punctuation:\n",
    "        filter_text.append(w)\n",
    "wordDict = dict.fromkeys(filter_text, 0)\n",
    "for w in filter_text:\n",
    "    wordDict[w] += 1\n",
    "# print(wordDict)\n",
    "tf = TermFreq(wordDict, filter_text)\n",
    "sorted_tf = sorted(tf.items(), key=operator.itemgetter(1), reverse=True)[:]\n",
    "# print(sorted_tf)\n",
    "senDict = dict.fromkeys(sentenses, 0)\n",
    "for s in sentenses:\n",
    "    tok = word_tokenize(s)\n",
    "    weight = 0\n",
    "    for w in tok:\n",
    "        if (w in tf.keys()):\n",
    "            weight += tf[w]\n",
    "    senDict[s] = weight\n",
    "# print(senDict)\n",
    "sorted_sd = sorted(senDict.items(), key=operator.itemgetter(1), reverse=True)[:]\n",
    "# print(sorted_sd)\n",
    "for i in sorted_sd:\n",
    "    essays = i[0]\n",
    "    tokens = nltk.word_tokenize(essays.lower())\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    a = withblank(tagged)\n",
    "    if a is not None:\n",
    "        print(a)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_spacenizer(x):\n",
    "    length = len(x)\n",
    "    \n",
    "    # Starting from \"of\" following \"NN\"\n",
    "    for i in range(length):\n",
    "        if  x[i][0] == \"of\" and x[i+1][1] == \"NN\":\n",
    "            return (i+1)\n",
    "        \n",
    "    # Starting from \"JJ\"\n",
    "    for i in range(length):\n",
    "        if i+1 < length and x[i][1] == \"JJ\":\n",
    "            if i+2 < length and x[i+1][1] == \"CC\":\n",
    "                if x[i+2][1] == \"JJ\":\n",
    "                    return i,(i+2)\n",
    "                \n",
    "            if i+2 < length and x[i+1][1] == \"NN\":\n",
    "                if i+3 < length and x[i+2][1] == \"CC\":\n",
    "                    if i+4 < length and x[i+3][1] == \"JJ\": \n",
    "                        if (x[i+4][1] == \"NN\" or x[i+4][1] == \"NNS\"):\n",
    "                            return i,i+1,i+3,i+4\n",
    "                    if x[i+3][1] == \"NNS\":\n",
    "                        return i,i+1,i+3\n",
    "                \n",
    "                if x[i+2][1] == \"NN\" or x[i+2][1] == \"NNS\":\n",
    "                        return i,i+1,i+2\n",
    "                \n",
    "                return i,i+1\n",
    "    \n",
    "    for i in range(length):\n",
    "        if i+2 < length and x[i][0] == \"referred\" and (x[i+1][0] == \"to\" or x[i+1][0] == \"as\"):\n",
    "            if i+3 < length and x[i+2][1] == \"NN\":\n",
    "                if x[i+3][1] == \"NN\":\n",
    "                    return i+2,i+3\n",
    "                else:\n",
    "                    return i+2\n",
    "                \n",
    "        if i+2 < length and x[i][0] == \"known\" and x[i+1][0] == \"as\":\n",
    "            if i+3 < length and x[i+2][1] == \"NN\":\n",
    "                if x[i+3][1] == \"NN\":\n",
    "                    return i+2,i+3\n",
    "                else:\n",
    "                    return i+2\n",
    "                \n",
    "# returns sentence with blanks\n",
    "def withblank(tagged):\n",
    "    y = sentence_spacenizer(tagged)\n",
    "    if y == None:\n",
    "        return\n",
    "    if type(y) == type(1):\n",
    "        sen = \"\"\n",
    "        for i in range(len(tagged)):\n",
    "            if i != y:\n",
    "                sen += tagged[i][0] + \" \"\n",
    "            else:\n",
    "                sen += \"_________\" + \" \"\n",
    "    else:\n",
    "        sen = \"\"\n",
    "        for i in range(len(tagged)):\n",
    "            if i not in y:\n",
    "                sen += tagged[i][0] + \" \"\n",
    "            else:\n",
    "                sen += \"_________\" + \" \"\n",
    "        return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likewise , in computer science , if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems , then it is said to have _________ _________ . \n",
      "analyze the problem and see the order in which the sub-problems are solved and start solving from the _________ _________ , up towards the given problem . \n",
      "if sub-problems can be nested recursively inside larger problems , so that _________ _________ _________ are applicable , then there is a relation between the value of the larger problem and the values of the sub-problems . \n",
      "also , the optimal solutions to the subproblems contribute to the _________ _________ of the given problem . \n",
      "if you see that the problem has been solved already , then just return the _________ _________ . \n",
      "in both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a _________ _________ . \n",
      "_________ _________ is both a mathematical optimization method and a computer programming method . \n",
      "this is referred to as _________ _________ . \n"
     ]
    }
   ],
   "source": [
    "lst = \"\"\n",
    "with open(\"data.txt\",\"r\") as f:\n",
    "    lst = f.read().lower()\n",
    "\n",
    "# lst = \"Analyze the problem and see the order in which the sub-problems are solved and start solving from the trivial subproblem, up towards the given problem. In this process, it is guaranteed that the subproblems are solved before solving the problem and thus, it is referred to as Dynamic Programming. Start solving the given problem by breaking it down. If you see that the problem has been solved already, then just return the saved answer. If it has not been solved, solve it and save the answer. This is usually easy to think of and very intuitive and thus, it is referred to as Memoization. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. Also, the optimal solutions to the subproblems contribute to the optimal solution of the given problem. There are two ways of doing this. Start solving the given problem by breaking it down. If you see that the problem has been solved already, then just return the saved answer. If it has not been solved, solve it and save the answer. This is usually easy to think of and very intuitive. This is referred to as Memoization. Analyze the problem and see the order in which the sub-problems are solved and start solving from the trivial subproblem, up towards the given problem.If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems. In the optimization literature this relationship is called the Bellman equation. Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure. \"\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import operator\n",
    "import string\n",
    "\n",
    "\n",
    "def TermFreq(wordDict, text):\n",
    "    tfDict = {}\n",
    "    totlen = len(text)\n",
    "    for w, cnt in wordDict.items():\n",
    "        tfDict[w] = cnt / float(totlen)\n",
    "    return tfDict                           \n",
    "\n",
    "f = lst\n",
    "content = f.lower()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"’\")\n",
    "data = word_tokenize(content)\n",
    "sentenses = sent_tokenize(content)\n",
    "# for i in sentenses:\n",
    "#     tokens = nltk.word_tokenize(i)\n",
    "#     tagged = nltk.pos_tag(tokens)\n",
    "#     print(tagged)\n",
    "filter_text = []\n",
    "for w in data:\n",
    "    if w not in stop_words and w not in string.punctuation:\n",
    "        filter_text.append(w)\n",
    "wordDict = dict.fromkeys(filter_text, 0)\n",
    "for w in filter_text:\n",
    "    wordDict[w] += 1\n",
    "# print(wordDict)\n",
    "tf = TermFreq(wordDict, filter_text)\n",
    "sorted_tf = sorted(tf.items(), key=operator.itemgetter(1), reverse=True)[:]\n",
    "# print(sorted_tf)\n",
    "senDict = dict.fromkeys(sentenses, 0)\n",
    "for s in sentenses:\n",
    "    tok = word_tokenize(s)\n",
    "    weight = 0\n",
    "    for w in tok:\n",
    "        if (w in tf.keys()):\n",
    "            weight += tf[w]\n",
    "    senDict[s] = weight\n",
    "# print(senDict)\n",
    "sorted_sd = sorted(senDict.items(), key=operator.itemgetter(1), reverse=True)[:]\n",
    "# print(sorted_sd)\n",
    "for i in sorted_sd:\n",
    "    essays = i[0]\n",
    "    tokens = nltk.word_tokenize(essays.lower())\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    a = withblank(tagged)\n",
    "    if a is not None:\n",
    "        print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
